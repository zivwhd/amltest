#!/bin/bash

## resource allocation
#SBATCH --job-name=FT_LLAMA_
#SBATCH --output=%x-%j.out
#SBATCH --error=%x-%j.err

#SBATCH --qos=gpu

#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=30g
#SBATCH --gpus=1
#SBATCH --nodelist=gpu8

#SBATCH --no-requeue


## modules and apps
module load anaconda3
source activate NEEA

## run
python fine_tune_llama.py